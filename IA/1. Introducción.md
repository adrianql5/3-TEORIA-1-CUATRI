
# 1.1 Definición de Inteligencia Artificial
La inteligencia artificial no tiene una definición universalmente aceptada. Una de las más utilizadas es la propuesta por la Comisión Europea, que entiende la IA como sistemas capaces de mostrar un comportamiento inteligente, analizando el entorno y tomando decisiones con cierto grado de autonomía para alcanzar objetivos específicos. Estos sistemas pueden estar basados únicamente en software o encontrarse embebidos en dispositivos de hardware.

Desde un punto de vista académico, también se puede considerar que un sistema es inteligente cuando es capaz de desenvolverse de forma autónoma en entornos complejos, posee una gran capacidad de aprendizaje y demuestra una competencia destacada en un área de conocimiento especializada.

# 1.2 Contexto histórico
El campo de la inteligencia artificial nació oficialmente en 1956. Se trata de una disciplina a medio camino entre la ciencia y la ingeniería: es ciencia porque busca ampliar el conocimiento, pero utiliza las herramientas de la ingeniería para dar soluciones prácticas a problemas concretos. Ya en 1958, un artículo del New York Times especulaba con que los ordenadores pudieran andar, hablar, ver, escribir, reproducirse e incluso ser conscientes de su propia existencia. En la actualidad se han conseguido grandes avances en los cuatro primeros aspectos, aunque apenas se ha progresado en los dos últimos.

El desarrollo de la IA no ha sido continuo, sino que ha atravesado periodos de estancamiento conocidos como “inviernos de la IA”. El avance llegó cuando aumentó la capacidad de cálculo de los ordenadores y se desarrollaron lenguajes de programación más potentes. Hoy la investigación se centra en el aprendizaje autónomo mediante grandes volúmenes de datos, y conviene destacar que la mayor parte de este trabajo se está llevando a cabo en el sector privado.

# 1.3 Comparación entre cerebro y ordenador
Cuando se compara el cerebro humano con los ordenadores actuales, resulta evidente la enorme capacidad y sobre todo la eficiencia del primero. El cerebro, a pesar de tener un peso reducido en relación al resto del cuerpo, consume aproximadamente un 20% de la energía. Aunque esta cifra pueda parecer elevada, en comparación con el consumo energético de las máquinas modernas es mínima, lo que pone de manifiesto que el cerebro es mucho más eficiente que cualquier sistema artificial desarrollado hasta ahora.

![[Pasted image 20250922101254.png]]

# 1.4 Enfoques de la Inteligencia Artificial
La IA ha pasado por distintos enfoques a lo largo de su historia. En un inicio, se buscaba reproducir el pensamiento humano en dispositivos electrónicos, siguiendo un enfoque cognitivo que no ofreció grandes resultados. Más adelante, el interés se dirigió hacia la creación de máquinas capaces de razonar de manera lógica, sin necesidad de imitar el razonamiento humano. Posteriormente surgió la idea de imitar directamente el comportamiento humano, lo que dio lugar a iniciativas como el Test de Turing o a los avances en robótica. Actualmente, el enfoque más extendido es el que persigue comportamientos racionales: resolver problemas de manera eficiente sin necesidad de imitar ni el pensamiento ni el comportamiento humano.

# 1.5 Alan Turing y sus aportaciones
Alan Turing fue un matemático y científico de la computación cuyas contribuciones resultaron fundamentales incluso antes de que existiera el campo de la inteligencia artificial como tal. En 1936 presentó la máquina de Turing, un modelo formal que representa el funcionamiento de un ordenador de propósito general. Años más tarde, en 1950, publicó un célebre artículo en el que planteaba la pregunta: “¿Pueden las máquinas pensar?”. Allí propuso el conocido Test de Turing, según el cual, si una máquina mantiene una conversación con una persona y logra confundirla al menos un 30% de las veces, puede considerarse que piensa.

La reflexión que dejó este planteamiento es profunda: ¿debemos reconocer como inteligente a una máquina simplemente porque imita el comportamiento humano, independientemente de cómo esté construida? Esta cuestión sigue abierta y continúa alimentando el debate sobre la naturaleza de la inteligencia artificial.

![[Pasted image 20250922101316.png]]



## 1.5.1. ¿Qué es pensar?
Para ilustrar la dificultad de definir qué significa “pensar”, Turing y otros autores propusieron un ejemplo. Imaginemos a una persona encerrada en una habitación con un repositorio infinito de información. Esa persona recibe textos en un idioma desconocido y responde con traducciones utilizando los datos almacenados, aunque no entiende realmente lo que escribe. ¿Se puede afirmar que esa persona conoce los idiomas? La respuesta inicial es que no, ya que no los habla ni los comprende; simplemente asocia símbolos. Por analogía, las máquinas tampoco serían inteligentes, pues solo realizan asociaciones a partir de la información disponible. Sin embargo, existen contraargumentos: desde un punto de vista utilitario, esa persona sí puede comunicarse en los distintos idiomas, lo cual, de hecho, equivale a “saberlos”.

## 1.5.2 Test de Turing
En la formulación del test participan tres actores: un juez, un humano y una máquina. El juez mantiene conversaciones con ambos mediante un sistema que elimina las pistas físicas como la voz o la caligrafía, por ejemplo, un chat escrito. Al terminar, debe decidir quién es el humano y quién la máquina. Según Turing, si el juez se equivoca y confunde a la máquina con el humano, entonces puede decirse que la máquina piensa.

### 1.5.3 Máquina de Turing
Años antes, en 1936, Turing había diseñado la llamada máquina de Turing, un modelo teórico con la misma capacidad de resolución de problemas computables que los ordenadores actuales. Conviene subrayar, no obstante, que esta capacidad no implica la misma eficiencia: la máquina de Turing es conceptualmente más simple, pero también mucho menos eficiente y más compleja de aplicar en la práctica.

## 1.5.4 Máquina Enigma
Durante la Segunda Guerra Mundial, el ejército nazi utilizaba la máquina Enigma para cifrar sus comunicaciones. Este dispositivo funcionaba con cilindros que rotaban con cada pulsación de tecla, de manera que el texto se encriptaba en función de su posición. Desencriptar los mensajes requería conocer la posición inicial de los cilindros, algo casi imposible sin ayuda externa. El ejército británico organizó un equipo de criptógrafos y lingüistas entre los que se encontraba Turing, que desarrolló “The Bombe”, una de las primeras supercalculadoras (aunque aún no considerada un ordenador completo). Gracias a esta máquina, fue posible descifrar los mensajes alemanes, acortando la guerra aproximadamente dos años y salvando miles de vidas.

# 1.6 La IA a lo largo de la historia

La evolución de la inteligencia artificial ha estado marcada por los llamados “inviernos de la IA”, periodos de estancamiento en los que la investigación se ralentizó o incluso se abandonó. En un primer momento se trabajó con inteligencias artificiales bioinspiradas o subsimbólicas, basadas en modelos de neuronas, pero las limitaciones tecnológicas provocaron el primer invierno. Posteriormente resurgió el interés gracias a las IA simbólicas, aunque la falta de resultados y el fracaso de costosos proyectos en superordenadores llevaron a un segundo invierno. En la actualidad, el campo vive un gran desarrollo impulsado por las multinacionales, y el foco ha regresado en gran medida a los modelos subsimbólicos bioinspirados.

![[Pasted image 20250922104050.png]]


# 1.7. Diferentes aproximaciones a la IA

## 1.7.1 IA subsimbólica

Este enfoque busca imitar el cerebro humano a través de modelos neuronales. Ramón y Cajal descubrió que las neuronas son integradores espacio-temporales que pueden estar activas o inactivas, y que transmiten señales a otras neuronas formando circuitos complejos. A nivel artificial, estas neuronas se modelan con entradas ponderadas, una función de suma, una función de activación y una salida.

![[Pasted image 20250922104155.png]]

Cuando se combinan muchas de estas unidades se crean redes neuronales con distintas capas: la capa de entrada (por ejemplo, los píxeles de una radiografía), la capa de salida (que podría indicar la presencia de un tumor) y varias capas intermedias que funcionan como una “caja negra”. Estas últimas representan un reto para la comprensión humana, ya que resulta difícil saber qué patrones están reconociendo. Este problema ha dado lugar a un nuevo campo llamado _explicabilidad_, que busca interpretar qué significan dichas capas.

![[Pasted image 20250922104213.png]]

El entrenamiento de estas redes comienza con pesos aleatorios en las conexiones. A través de datos etiquetados se van ajustando estos pesos hasta obtener un modelo capaz de resolver el problema planteado. Este proceso se conoce como aprendizaje supervisado.

## 1.7.2 IA simbólica

A diferencia de la anterior, la inteligencia artificial simbólica no imita la estructura neuronal, sino la funcionalidad del cerebro mediante la manipulación de símbolos. Se basa en programación tradicional y en la definición explícita de reglas. Los símbolos reciben significados concretos y se establece, a través de un lenguaje de programación, cómo debe actuar la máquina frente a los estímulos recibidos.

![[Pasted image 20250922104252.png]]


# 1.8 Hitos de la inteligencia artificial
## 1.8.1 Deep Blue contra Kasparov
En 1997, la IA simbólica _Deep Blue_, desarrollada por IBM, venció al campeón mundial de ajedrez Garry Kasparov. Este hecho causó gran impresión, pues dio la sensación de que las máquinas se acercaban a la inteligencia humana. Sin embargo, se trataba de un logro limitado: Deep Blue no “pensaba”, simplemente evaluaba millones de jugadas posibles. Hoy en día, redes neuronales subsimbólicas como las de _DeepMind_ han superado a Deep Blue con menos de 40 horas de entrenamiento.

## 1.8.2 AlphaGo contra Lee Sedol

DeepMind también desarrolló _AlphaGo_, una red neuronal subsimbólica que logró vencer a Lee Sedol, uno de los mejores jugadores de Go de la historia. El Go es mucho más complejo que el ajedrez por la cantidad casi infinita de combinaciones posibles. AlphaGo demostró un enorme potencial, aunque también puso de manifiesto una limitación importante: estas redes no son extrapolables, es decir, un sistema entrenado para un juego específico no puede aplicarse con éxito a otros problemas diferentes.


# 1.9 Paradoja de Moravec
Hans Moravec formuló una paradoja sorprendente: lo que parece más difícil para los humanos —el razonamiento lógico y abstracto— requiere relativamente poca computación en una máquina. En cambio, lo que para nosotros es natural e inconsciente —como reconocer objetos o coordinar movimientos— resulta extremadamente costoso en términos computacionales.


# 1.10 El problema de los mínimos locales
En el entrenamiento de redes neuronales subsimbólicas se utiliza la función derivada del error para ajustar los pesos de las conexiones. Sin embargo, puede suceder que esta función alcance un mínimo local, es decir, un punto en el que el error no disminuye más pero que no es necesariamente el mejor resultado posible (mínimo absoluto). Este problema sigue siendo objeto de investigación, ya que dificulta optimizar plenamente el aprendizaje de los modelos.


